<!DOCTYPE html><html lang="en-us" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  
  
  
  <meta name="generator" content="Wowchemy 4.8.0 for Hugo">
  

  

  
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Vision and Language Group">

  
  
  
    
  
  <meta name="description" content="Introduction This repo houses summaries for various excitng works in the field of Deep Learning. You can contribute summaries of your own. Check out our contributing guide to start contributing. Happy Reading &amp; Summarizing!">

  
  <link rel="alternate" hreflang="en-us" href="https://vlgiitr.github.io/summaries/">

  







  




  
  
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  

  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  

  
  
  
  
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.2.2/lazysizes.min.js" integrity="sha512-TmDwFLhg3UA4ZG0Eb4MIyT1O1Mb+Oww5kFG0uHqXsdbyZz9DcvYQhKpGgNkamAI6h2lGGZq2X8ftOJvF/XjTUg==" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      
        
      

      
    
      

      
      

      
    
      

      
      

      
    

  

  
  
  
    
      
      
      <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
    
  

  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.css">

  




  


  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu0af03150d0ca39f3b12fa58639b44cf7_60645_32x32_fill_lanczos_center_3.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu0af03150d0ca39f3b12fa58639b44cf7_60645_192x192_fill_lanczos_center_3.png">

  <link rel="canonical" href="https://vlgiitr.github.io/summaries/">

  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="twitter:site" content="@vlgiitr">
  <meta property="twitter:creator" content="@vlgiitr">
  
  <meta property="og:site_name" content="VLG">
  <meta property="og:url" content="https://vlgiitr.github.io/summaries/">
  <meta property="og:title" content="Papers We Read | VLG">
  <meta property="og:description" content="Introduction This repo houses summaries for various excitng works in the field of Deep Learning. You can contribute summaries of your own. Check out our contributing guide to start contributing. Happy Reading &amp; Summarizing!"><meta property="og:image" content="https://vlgiitr.github.io/images/logo_hu0af03150d0ca39f3b12fa58639b44cf7_60645_300x300_fit_lanczos_3.png">
  <meta property="twitter:image" content="https://vlgiitr.github.io/images/logo_hu0af03150d0ca39f3b12fa58639b44cf7_60645_300x300_fit_lanczos_3.png"><meta property="og:locale" content="en-us">
  
    
    
  

  



  


  


  





  <title>Papers We Read | VLG</title>

</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class=" ">

  
  
  
    <script>window.wcDarkLightEnabled = true;</script>
  
  
    <script>const isSiteThemeDark = false;</script>
  
  
  <script src="/js/load-theme.js"></script>

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  











  


<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/"><img src="/images/logo_hu0af03150d0ca39f3b12fa58639b44cf7_60645_0x70_resize_lanczos_3.png" alt="VLG"></a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/"><img src="/images/logo_hu0af03150d0ca39f3b12fa58639b44cf7_60645_0x70_resize_lanczos_3.png" alt="VLG"></a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link  active" href="/summaries/"><span>Papers We Read</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Blogs</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/team/"><span>Team</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/alums/"><span>Alumni</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true">
          <i class="fas fa-moon" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      

    </ul>

  </div>
</nav>



  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Papers We Read</h1>

  

  
    


<div class="article-metadata">

  
  

  

  

  

  
  
  

  
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      <h2 id="introduction">Introduction</h2>
<p>This repo houses summaries for various excitng works in the field of <strong>Deep Learning</strong>. You can contribute summaries of your own. Check out our <a href="#contributing">contributing guide</a> to start contributing. Happy Reading &amp; Summarizing!</p>
<h2 id="contents">Contents</h2>
<ul>
<li><a href="#summaries">Summaries</a>
<ul>
<li><a href="#2022">2022</a></li>
<li><a href="#2021">2021</a></li>
<li><a href="#2020">2020</a></li>
<li><a href="#2019">2019</a></li>
<li><a href="#2018">2018</a></li>
<li><a href="#2017">2017</a></li>
<li><a href="#2016">2016</a></li>
</ul>
</li>
<li><a href="#contributing">Contributing</a></li>
<li><a href="#acknowledgements">Acknowledgements</a></li>
<li><a href="#license">License</a></li>
</ul>
<h2 id="summaries">Summaries</h2>
<h3 id="2022">2022</h3>
<ul>
<li>
<h4 id="human-level-play-in-the-game-of-diplomacy-by-combining-language-models-with-strategic-reasoning-paperhttpswwwscienceorgdoiepdf101126scienceade9097reviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariesciceromd">Human-level play in the game of Diplomacy by combining language models with strategic reasoning [<a href="https://www.science.org/doi/epdf/10.1126/science.ade9097" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/CICERO.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Meta Fundamental AI Research Diplomacy Team (FAIR), Antin Bakhtun, Noam Brown, Emily Dinan, <strong>Science Journal 2022</strong></li>
</ul>
</li>
<li>
<h4 id="photorealistic-text-to-image-diffusion-models-with-deep-language-understanding-paperhttpsarxivorgabs220511487reviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariesimagenmd">Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding [<a href="https://arxiv.org/abs/2205.11487" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/imagen.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, Mohammad Norouzi, <strong>NIPS 2022</strong></li>
</ul>
</li>
<li>
<h4 id="learning-video-representations-from-large-language-models-paperhttpsarxivorgpdf221204501pdfreviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariesvideo_representation_llmmd">Learning Video Representations from Large Language Models [<a href="https://arxiv.org/pdf/2212.04501.pdf" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/Video_Representation_LLM.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Yue Zhao, Ishan Misra, Philipp Krähenbüh, Rohit Girdhar, Facebook AI Research- Meta AI, University of Texas, Austin</li>
</ul>
</li>
</ul>
<h3 id="2021">2021</h3>
<ul>
<li>
<h4 id="gancraft-unsupervised-3d-neural-rendering-of-minecraft-worlds-paperhttpsarxivorgpdf210407659reviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariesgancraftmd">GANcraft: Unsupervised 3D Neural Rendering of Minecraft Worlds [<a href="https://arxiv.org/pdf/2104.07659" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/GANcraft.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Zekun Hao, Arun Mallya, Serge Belongie, Ming-Yu Liu, <strong>ICCV 2021</strong></li>
</ul>
</li>
<li>
<h4 id="giraffe-representing-scenes-as-compositional-generative-neural-feature-fields-paperhttpsarxivorgpdf201112100reviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariesgiraffemd">GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields [<a href="https://arxiv.org/pdf/2011.12100" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/GIRAFFE.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Michael Niemeyer, Andreas Geiger, <strong>CVPR 2021</strong></li>
</ul>
</li>
<li>
<h4 id="creative-sketch-genetation-paperhttpsarxivorgabs201110039reviewhttpsgithubcomsandstorm831papers_we_readblobmastersummariesdoodlergan20summarymd">Creative Sketch Genetation [<a href="https://arxiv.org/abs/2011.10039" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/Sandstorm831/papers_we_read/blob/master/summaries/DoodlerGAN%20summary.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Songwei Ge, Devi Parikh, Vedanuj Goswami &amp; C. Lawrence Zitnick, <strong>ICLR 2021</strong></li>
</ul>
</li>
<li>
<h4 id="binary-ttc-a-temporal-geofence-for-autonomous-navigationpaperhttpsarxivorgabs210104777reviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariesbinary_ttcmd">Binary TTC: A Temporal Geofence for Autonomous Navigation[<a href="https://arxiv.org/abs/2101.04777" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/binary_TTC.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Abhishek Badki, Orazio Gallo, Jan Kautz, Pradeep Sen, <strong>CVPR 2021</strong></li>
</ul>
</li>
</ul>
<h3 id="2020">2020</h3>
<ul>
<li>
<h4 id="unsupervised-learning-of-probably-symmetric-deformable-3d-objects-from-images-in-the-wild-paperhttpsarxivorgabs191111130reviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariesunsupervised_learning_for_3d_objects_from_imagesmd">Unsupervised Learning of Probably Symmetric Deformable 3D Objects from Images in the Wild [<a href="https://arxiv.org/abs/1911.11130" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/Unsupervised_learning_for_3D_objects_from_images.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Shangzhe Wu, Christian Rupprecht, Andrea Vedaldi, <strong>CVPR 2020</strong></li>
</ul>
</li>
<li>
<h4 id="you-only-train-once-loss-conditional-training-of-deep-networks-paperhttpsopenreviewnetpdfidhyxy6jhkwrreviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariesyou_only_train_oncemd">You Only Train Once: Loss-conditional training of deep networks [<a href="https://openreview.net/pdf?id=HyxY6JHKwr" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/You_only_train_once.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Alexey Dosovitskiy, Josip Djolonga, <strong>ICLR 2020</strong></li>
</ul>
</li>
<li>
<h4 id="groknet-unified-computer-vision-model-trunk-and-embeddings-for-commerce-paperhttpsaifacebookcomresearchpublicationsgroknet-unified-computer-vision-model-trunk-and-embeddings-for-commercereviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariesgroknetmd">GrokNet: Unified Computer Vision Model Trunk and Embeddings For Commerce [<a href="https://ai.facebook.com/research/publications/groknet-unified-computer-vision-model-trunk-and-embeddings-for-commerce" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/GrokNet.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Sean Bell, Yiqun Liu, Sami Alsheikh, Yina Tang, Ed Pizzi, M. Henning, Karun Singh, Omkar Parkhi, Fedor Borisyuk, <strong>KDD 2020</strong></li>
</ul>
</li>
<li>
<h4 id="semantically-multi-modal-image-synthesis-paperhttpsarxivorgabs200312697reviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariessemantically_multi-modal_image_synthesismd">Semantically multi-modal image synthesis [<a href="https://arxiv.org/abs/2003.12697" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/Semantically_multi-modal_image_synthesis.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Zhen Zhu, Zhiliang Xu, Ansheng You, Xiang Bai, <strong>CVPR 2020</strong></li>
</ul>
</li>
<li>
<h4 id="learning-to-simulate-dynamic-environments-with-gamegan-paperhttparxivorgabs200512126reviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariesgameganmd">Learning to Simulate Dynamic Environments with GameGAN [<a href="http://arxiv.org/abs/2005.12126" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/GameGAN.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Seung Wook Kim, Yuhao Zhou, Jonah Philion, Antonio Torralba, Sanja Fidler, <strong>CVPR 2020</strong></li>
</ul>
</li>
<li>
<h4 id="adversarial-policies--attacking-deep-reinforcement-learning-paperhttpsarxivorgabs190510615reviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariesadversarial_rlmd">Adversarial Policies : Attacking deep reinforcement learning [<a href="https://arxiv.org/abs/1905.10615" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/Adversarial_RL.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, Stuart Russell, <strong>ICLR 2020</strong></li>
</ul>
</li>
<li>
<h4 id="bootstrap-your-own-latent-a-new-approach-to-self-supervised-learning-paperhttpsarxivorgabs200607733reviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariesbyolmd">Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning [<a href="https://arxiv.org/abs/2006.07733" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/BYOL.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Rémi Munos, Michal Valko, <strong>CVPR 2020</strong></li>
</ul>
</li>
</ul>
<h3 id="2019">2019</h3>
<ul>
<li>
<h4 id="vilbert-pretraining-task-agnostic-visiolinguistic-representations-for-vision-and-language-tasks-paperhttpsarxivorgabs190802265reviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariesvilbertmd">ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks [<a href="https://arxiv.org/abs/1908.02265" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/ViLBERT.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee, <strong>NIPS 2019</strong></li>
</ul>
</li>
<li>
<h4 id="stand-alone-self-attention-in-vision-models-paperhttpsarxivorgabs190605909reviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariesvision_attentionmd">Stand-Alone Self-Attention in Vision Models [<a href="https://arxiv.org/abs/1906.05909" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/vision_attention.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, Jonathon Shlens, <strong>NIPS 2019</strong></li>
</ul>
</li>
<li>
<h4 id="zero-shot-entity-linking-by-reading-entity-descriptions-paperhttpsarxivorgabs190607348reviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariesentity_linkingmd">Zero-Shot Entity Linking by Reading Entity Descriptions [<a href="https://arxiv.org/abs/1906.07348" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/entity_linking.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Lajanugen Logeswaran ,  Ming-Wei Chang‡ Kenton Lee ,  Kristina Toutanova , Jacob Devlin, Honglak Lee <strong>ACL-2019</strong></li>
</ul>
</li>
<li>
<h4 id="do-you-know-that-florence-is-packed-with-visitors-evaluating-state-of-the-art-models-of-speaker-commitment-paperhttpswwwaclweborganthologyp19-1412reviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariesflorencemd">Do you know that Florence is packed with visitors? Evaluating state-of-the-art models of speaker commitment [<a href="https://www.aclweb.org/anthology/P19-1412/" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/florence.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Nanjiang Jiang and Marie-Catherine de Marneffe , <strong>ACL-2019</strong></li>
</ul>
</li>
<li>
<h4 id="scene-representation-networks-continuous-3d-structure-aware-neural-scene-representations-paperhttpspapersnipsccpaper8396-scene-representation-networks-continuous-3d-structure-aware-neural-scene-representationspdfreviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariessrnmd">Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations [<a href="https://papers.nips.cc/paper/8396-scene-representation-networks-continuous-3d-structure-aware-neural-scene-representations.pdf" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/srn.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Vincent Sitzmann, Michael Zollhofer, Gordon Wetzstein, <strong>NIPS-2019</strong></li>
</ul>
</li>
<li>
<h4 id="emotion-cause-pair-extraction-a-new-task-to-emotion-analysis-in-texts-paperhttpsarxivorgabs190601267reviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariesecpemd">Emotion-Cause Pair Extraction: A New Task to Emotion Analysis in Texts [<a href="https://arxiv.org/abs/1906.01267" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/ecpe.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Rui Xia, Zixiang Ding, <strong>ACL-2019</strong></li>
</ul>
</li>
<li>
<h4 id="putting-an-end-to-end-to-end-gradient-isolated-learning-of-representations-paperhttpspapersnipsccpaper8568-putting-an-end-to-end-to-end-gradient-isolated-learning-of-representationspdfreviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariesinfomaxmd">Putting an End to End-to-End: Gradient-Isolated Learning of Representations [<a href="https://papers.nips.cc/paper/8568-putting-an-end-to-end-to-end-gradient-isolated-learning-of-representations.pdf" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/infomax.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Sindy Lowe, Peter O&rsquo; Connor, Bastiaan S. Veeling, <strong>NIPS-2019</strong></li>
</ul>
</li>
<li>
<h4 id="bridging-the-gap-between-training-and-inference-for-neural-machine-translation-paperhttpsarxivorgabs190602448reviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariesnmt_gapmd">Bridging the Gap between Training and Inference for Neural Machine Translation [<a href="https://arxiv.org/abs/1906.02448" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/NMT_Gap.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Wen Zhang, Yang Feng, Fandong Meng, Di You, Qun Liu, <strong>ACL-2019</strong></li>
</ul>
</li>
<li>
<h4 id="designing-and-interpreting-probes-with-control-tasks-paperhttpsarxivorgabs190903368reviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariescontrol_tasksmd">Designing and Interpreting Probes with Control Tasks [<a href="https://arxiv.org/abs/1909.03368" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/control_tasks.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>John Hewitt, Percy Liang, <strong>EMNLP-2019</strong></li>
</ul>
</li>
<li>
<h4 id="specializing-word-embeddings-for-parsing-by-information-bottleneck-paperhttpsarxivorgabs191000163reviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariesinfo_bottleneckmd">Specializing Word Embeddings (for Parsing) by Information Bottleneck [<a href="https://arxiv.org/abs/1910.00163" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/info_bottleneck.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Xiang Lisa Li, Jason Eisner, <strong>EMNLP-2019</strong></li>
</ul>
</li>
<li>
<h4 id="vgraph-a-generative-model-for-joint-community-detection-and-node-representational-learning-paperhttpsarxivorgabs190607159reviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariesvgraphmd">vGraph: A Generative Model for Joint Community Detection and Node Representational Learning [<a href="https://arxiv.org/abs/1906.07159" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/vgraph.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Fan-Yun Sun, Meng Qu, Jordan Hoffmann, Chin-Wei Huang, Jian Tang, <strong>NIPS-2019</strong></li>
</ul>
</li>
<li>
<h4 id="uniform-convergence-may-be-unable-to-explain-generalization-in-deep-learning-paperhttpsarxivorgabs190204742reviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariesuniform_convergencemd">Uniform convergence may be unable to explain generalization in deep learning [<a href="https://arxiv.org/abs/1902.04742" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/uniform_convergence.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Vaishnavh Nagarajan, J. Zico Kolter, <strong>NIPS-2019</strong></li>
</ul>
</li>
<li>
<h4 id="singan-learning-a-generative-model-from-a-single-natural-image-paperhttpsarxivorgpdf190501164reviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariessinganmd">SinGAN: Learning a Generative Model from a Single Natural Image [<a href="https://arxiv.org/pdf/1905.01164" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/singan.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Tamar Rott Shaham, Tali Dekel, Tomer Michaeli, <strong>ICCV-2019</strong></li>
</ul>
</li>
<li>
<h4 id="graph-u-nets-paperhttpsarxivorgabs190505178reviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariesgraph_unetmd">Graph U-Nets [<a href="https://arxiv.org/abs/1905.05178" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/graph_unet.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Hongyang Gao, Shuiwang Ji, <strong>ICML-2019</strong></li>
</ul>
</li>
<li>
<h4 id="feature-denoising-for-improving-adversarial-robustness-paperhttpsarxivorgpdf181203411reviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariesfeature_denoisingmd">Feature Denoising for Improving Adversarial Robustness [<a href="https://arxiv.org/pdf/1812.03411" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/feature_denoising.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan Yuille, kaiming He, <strong>CVPR-2019</strong></li>
</ul>
</li>
<li>
<h4 id="this-looks-like-that-deep-learning-for-interpretable-image-recognition-paperhttpsarxivorgpdf180610574pdfreviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariesthis_looks_like_thatmd">This Looks Like That: Deep Learning for Interpretable Image Recognition [<a href="https://arxiv.org/pdf/1806.10574.pdf" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/this_looks_like_that.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Chaofan Chen, Oscar Li, Chaofan Tao, Alina Jade Barnett, Jonathan Su, Cynthia Rudin, <strong>NIPS-2019</strong></li>
</ul>
</li>
</ul>
<h3 id="2018">2018</h3>
<ul>
<li>
<h4 id="cycada-cycle-consistent-adversarial-domain-adaptation-paperhttpsarxivorgpdf171103213pdfreviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariescycadamd">CyCADA: Cycle-Consistent Adversarial Domain Adaptation [<a href="https://arxiv.org/pdf/1711.03213.pdf" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/cycada.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei A. Efros, Trevor Darrell, <strong>ICML-2018</strong></li>
</ul>
</li>
</ul>
<h3 id="2017">2017</h3>
<ul>
<li>
<h4 id="unpaired-image-to-image-translation-using-cycle-consistent-adversarial-networks-paperhttpsarxivorgabs170310593reviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariescycleganmd">Unpaired Image-to-Image Translation using Cycle Consistent Adversarial Networks [<a href="https://arxiv.org/abs/1703.10593" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/cyclegan.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros, <strong>ICCV-2017</strong></li>
</ul>
</li>
<li>
<h4 id="densely-connected-convolutional-networks-paperhttpsarxivorgabs160806993reviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariesdensenetmd">Densely Connected Convolutional Networks [<a href="https://arxiv.org/abs/1608.06993" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/densenet.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Q. Weinberger, <strong>CVPR-2017</strong></li>
</ul>
</li>
<li>
<h4 id="on-calibration-of-modern-neural-networks-paperhttpsarxivorgabs170604599reviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariescalibration_of_neural_netsmd">On Calibration of Modern Neural Networks [<a href="https://arxiv.org/abs/1706.04599" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/Calibration_of_Neural_Nets.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q. Weinberger, <strong>ICML-2017</strong></li>
</ul>
</li>
</ul>
<h3 id="2016">2016</h3>
<ul>
<li>
<h4 id="siamese-recurrent-architectures-for-learning-sentence-similarity-paperhttpsdlacmorgcitationcfmid3016291reviewhttpsgithubcomvlgiitrpapers_we_readblobmastersummariessiamesemd">Siamese Recurrent Architectures for Learning Sentence Similarity [<a href="https://dl.acm.org/citation.cfm?id=3016291" target="_blank" rel="noopener">Paper</a>][<a href="https://github.com/vlgiitr/papers_we_read/blob/master/summaries/siamese.md" target="_blank" rel="noopener">Review</a>]</h4>
<ul>
<li>Jonas Mueller, Aditya Thyagarajan, <strong>AAAI-2016</strong></li>
</ul>
</li>
</ul>
<h2 id="contributing">Contributing</h2>
<p>We appreciate all contributions to the set of summaries. Please refer to <a href="CONTRIBUTING.md">CONTRIBUTING.md</a> for the contributing guideline.</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p><strong>papers_we_read</strong> is an open source repository that welcomes any contribution and feedback. We wish the collected sets of summaries can help the DL community to start with the practice of reading and understanding research papers which is a potent skill in the research community. Most of our <a href="https://github.com/vlgiitr/papers_we_read/graphs/contributors" target="_blank" rel="noopener">contributors</a> include students enrolled in undergraduate programmes. We are grateful for all the contributions that help improve this collection of summaries.</p>
<h2 id="license">License</h2>
<p>This repo is open-sourced under the <a href="LICENSE">MIT License</a>.</p>

    </div>

    








<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://vlgiitr.github.io/summaries/&amp;text=Papers%20We%20Read" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://vlgiitr.github.io/summaries/&amp;t=Papers%20We%20Read" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Papers%20We%20Read&amp;body=https://vlgiitr.github.io/summaries/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://vlgiitr.github.io/summaries/&amp;title=Papers%20We%20Read" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Papers%20We%20Read%20https://vlgiitr.github.io/summaries/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://vlgiitr.github.io/summaries/&amp;title=Papers%20We%20Read" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>

















  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks",
        'slides' : "Slides"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/wowchemy.min.ee9dd1b51c1cbfc9adeb51e02586011c.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    
  </p>

  
  






  <p class="powered-by">
    
    Published with
    <a href="https://wowchemy.com" target="_blank" rel="noopener">Wowchemy</a>  —
    the free, <a href="https://github.com/wowchemy/wowchemy-hugo-modules" target="_blank" rel="noopener">
    open source</a> website builder that empowers creators.
    

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
